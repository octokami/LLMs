{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinscaria/InstructABSA/blob/main/ATE_Training_%26_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l2xc1EQjq3LY"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment Name:  lapt2014_iabsa1\n",
            "Model output path:  ./Models\\ate\\allenaitk-instruct-base-def-pos-lapt2014_iabsa1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch  \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "\n",
        "from InstructABSA.data_prep import DatasetLoader, xml_to_dataframe\n",
        "from InstructABSA.utils import T5Generator, T5Classifier\n",
        "from instructions import InstructionsHandler\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount = True)\n",
        "    IN_COLAB = True\n",
        "    if IN_COLAB:\n",
        "        !pip install transformers\n",
        "        !pip install datasets\n",
        "        !pip install evaluate\n",
        "        !pip install sentencepiece\n",
        "\n",
        "        root_path = 'Enter drive path'\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    root_path = 'D:\\GitHub\\Thesis\\InstructABSA'\n",
        "\n",
        "use_mps = True if torch.has_mps else False\n",
        "os.chdir(root_path)\n",
        "\n",
        "task_name = 'ate'\n",
        "experiment_name = 'lapt2014_iabsa1'\n",
        "model_checkpoint = 'allenai/tk-instruct-base-def-pos'\n",
        "print('Experiment Name: ', experiment_name)\n",
        "model_out_path = './Models'\n",
        "model_out_path = os.path.join(model_out_path, task_name, f\"{model_checkpoint.replace('/', '')}-{experiment_name}\")\n",
        "print('Model output path: ', model_out_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ht-x1qxn_AxG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3hfAuLAvVe8D"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "id_train_file_path = r\"D:\\GitHub\\Thesis\\data\\raw\\ABSA16_Restaurants_Train_SB1_v2.xml\"\n",
        "id_test_file_path = r\"D:\\GitHub\\Thesis\\data\\raw\\EN_REST_SB1_TEST.xml.gold\"\n",
        "\n",
        "in_dir = \"data/\"\n",
        "#id_tr_df = pd.read_csv(id_train_file_path)\n",
        "#id_te_df = pd.read_csv(id_test_file_path)\n",
        "\n",
        "try: \n",
        "  trial_df = pd.read_parquet(os.path.join(in_dir, 'trial_data.parquet'))\n",
        "  id_tr_df = pd.read_parquet(os.path.join(in_dir, 'train_data.parquet'))\n",
        "  id_te_df = pd.read_parquet(os.path.join(in_dir, 'test_data.parquet'))\n",
        "except:\n",
        "  trial_df = xml_to_dataframe('https://alt.qcri.org/semeval2016/task5/data/uploads/trial-data/english-trial/restaurants_trial_english_sl.xml')\n",
        "  id_tr_df = xml_to_dataframe(id_train_file_path, output_file = 'train_data', in_dir = in_dir)\n",
        "  id_te_df = xml_to_dataframe(id_test_file_path, output_file = 'test_data', in_dir = in_dir)\n",
        "\n",
        "# Get the input text into the required format using Instructions\n",
        "instruct_handler = InstructionsHandler()\n",
        "\n",
        "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
        "instruct_handler.load_instruction_set1()\n",
        "\n",
        "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
        "loader = DatasetLoader(id_tr_df, id_te_df)\n",
        "if loader.train_df_id is not None:\n",
        "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
        "if loader.test_df_id is not None:\n",
        "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k4v8QMMT4B7t"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        }
      ],
      "source": [
        "# Create T5 utils object\n",
        "t5_exp = T5Generator(model_checkpoint)\n",
        "\n",
        "# Tokenize Dataset\n",
        "id_ds, id_tokenized_ds, ood_ds, ood_tokenized_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
        "\n",
        "# Training arguments\n",
        "training_args = {\n",
        "    'output_dir':model_out_path,\n",
        "    'evaluation_strategy':\"epoch\",\n",
        "    'learning_rate':5e-5,\n",
        "    'lr_scheduler_type':'cosine',\n",
        "    'per_device_train_batch_size':8,\n",
        "    'per_device_eval_batch_size':16,\n",
        "    'num_train_epochs':4,\n",
        "    'weight_decay':0.01,\n",
        "    'warmup_ratio':0.1,\n",
        "    'save_strategy':'no',\n",
        "    'load_best_model_at_end':False,\n",
        "    'push_to_hub':False,\n",
        "    'eval_accumulation_steps':1,\n",
        "    'predict_with_generate':True,\n",
        "    'use_mps_device':use_mps\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tZr7BAy6eJOA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer device: cpu\n",
            "\n",
            "Model training started ....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/176 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "                                                  \n",
            " 25%|██▌       | 44/176 [19:48<38:23, 17.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9263168573379517, 'eval_runtime': 89.6719, 'eval_samples_per_second': 1.004, 'eval_steps_per_second': 0.067, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 50%|█████     | 88/176 [36:33<24:19, 16.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.8029510378837585, 'eval_runtime': 84.5297, 'eval_samples_per_second': 1.065, 'eval_steps_per_second': 0.071, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 75%|███████▌  | 132/176 [51:18<10:56, 14.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7621347904205322, 'eval_runtime': 85.5681, 'eval_samples_per_second': 1.052, 'eval_steps_per_second': 0.07, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            "100%|██████████| 176/176 [1:06:13<00:00, 22.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7742469906806946, 'eval_runtime': 86.6359, 'eval_samples_per_second': 1.039, 'eval_steps_per_second': 0.069, 'epoch': 4.0}\n",
            "{'train_runtime': 3973.4549, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.044, 'train_loss': 1.139618700200861, 'epoch': 4.0}\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "model_trainer = t5_exp.train(id_tokenized_ds, **training_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7rqL4maz_Dlz"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cy6aOHv4_FUo"
      },
      "outputs": [],
      "source": [
        "# Get the input text into the required format using Instructions\n",
        "instruct_handler = InstructionsHandler()\n",
        "\n",
        "# Set instruction_set1 for InstructABSA-1 and instruction_set2 for InstructABSA-2\n",
        "instruct_handler.load_instruction_set1()\n",
        "\n",
        "# Set bos_instruct1 for lapt14 and bos_instruct2 for rest14. For other datasets, modify the insructions.py file.\n",
        "loader = DatasetLoader(id_tr_df, id_te_df)\n",
        "if loader.train_df_id is not None:\n",
        "    loader.train_df_id = loader.create_data_in_ate_format(loader.train_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])\n",
        "if loader.test_df_id is not None:\n",
        "    loader.test_df_id = loader.create_data_in_ate_format(loader.test_df_id, 'term', 'raw_text', 'aspectTerms', instruct_handler.ate['bos_instruct1'], instruct_handler.ate['eos_instruct'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kbcnbLXtt9Am"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded to:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22/22 [15:07<00:00, 41.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded to:  cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [04:44<00:00, 47.45s/it]\n"
          ]
        }
      ],
      "source": [
        "# Model inference - Loading from Checkpoint\n",
        "t5_exp = T5Generator(model_out_path)\n",
        "\n",
        "# Tokenize Datasets\n",
        "id_ds, id_tokenized_ds, ood_ds, ood_tokenzed_ds = loader.set_data_for_training_semeval(t5_exp.tokenize_function_inputs)\n",
        "\n",
        "# Get prediction labels - Training set   \n",
        "id_tr_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'train', batch_size = 16) #, trained_model_path = model_out_path\n",
        "id_tr_labels = [i.strip() for i in id_ds['train']['labels']]\n",
        "\n",
        "# Get prediction labels - Testing set\n",
        "id_te_pred_labels = t5_exp.get_labels(tokenized_dataset = id_tokenized_ds, sample_set = 'test', batch_size = 16) #, trained_model_path = model_out_path\n",
        "id_te_labels = [i.strip() for i in id_ds['test']['labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pBCUT9jDt8-Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Precision:  0.5277161862527716\n",
            "Train Recall:  0.7582636399840701\n",
            "Train F1:  0.6223239091354795\n",
            "Test Precision:  0.4310210444271239\n",
            "Test Recall:  0.6422764227642277\n",
            "Test F1:  0.5158582089552239\n"
          ]
        }
      ],
      "source": [
        "p, r, f1 = t5_exp.get_metrics(id_tr_labels, id_tr_pred_labels)\n",
        "print('Train Precision: ', p)\n",
        "print('Train Recall: ', r)\n",
        "print('Train F1: ', f1)\n",
        "\n",
        "p, r, f1 = t5_exp.get_metrics(id_te_labels, id_te_pred_labels)\n",
        "print('Test Precision: ', p)\n",
        "print('Test Recall: ', r)\n",
        "print('Test F1: ', f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "basegpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "070d57aa6b4a039a680ca3535d2f37da5ed020b02d8ccf58fedcd4e32b3636b0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
